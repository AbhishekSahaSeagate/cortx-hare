#!/usr/bin/env bash
set -eu -o pipefail
export PS4='+ [${BASH_SOURCE[0]##*/}:${LINENO}${FUNCNAME[0]:+:${FUNCNAME[0]}}] '
# set -x

PROG=${0##*/}

usage() {
    cat <<EOF
Usage: $PROG [OPTS] <CDF> [<params.yaml>]

Configures EES-HA by preparing the configuration files and
adding resources into the Pacemaker.

Here are some prerequisites/guidelines:

* Pacemaker should be started & configured with a
  clean (without resources) cluser. Check with 'pcs status'.

* Password-less ssh access between the nodes is required.

* The script should be run from the "left" node.

* Make sure the provided roaming IP addresses belong to
  the local sub-network and are not used by anyone else.

Mandatory parameters:
  --ip1 <addr>         1st roaming IP address
  --ip2 <addr>         2nd roaming IP address.
        <CDF>          Hare Cluster Description File

Optional parameters:
  -i, --interface <if>  Data network interface (default: eth1)
  --left-node     <n1>  Left  node hostname (default: pod-c1)
  --right-node    <n2>  Right node hostname (default: pod-c2)
  --left-volume   <lv>  Left  node /var/mero volume (default: /dev/sdb)
  --right-volume  <rv>  Right node /var/mero volume (default: /dev/sdc)
  --skip-mkfs           Don't mkfs /var/mero
  --net-type <tcp|o2ib> LNet network type (default: tcp)

Note: parameters can be specified either directly via cmd-line options
or via a yaml file, e.g.:

  ip1: <ip>
  ip2: <ip2>
  interface: <iface>
  left-node: <lnode>
  right-node: <rnode>
  left-volume: <lvolume>
  right-volume: <rvolume>
  skip-mkfs: true
  net-type: <tcp|o2ib>
EOF
}

TEMP=$(getopt --options h,i: \
              --longoptions help,ip1:,ip2:,interface:,left-node:,right-node: \
              --longoptions left-volume:,right-volume:,skip-mkfs,net-type: \
              --name "$PROG" -- "$@" || true)

(($? == 0)) || { usage >&2; exit 1; }

eval set -- "$TEMP"

ip1=
ip2=
iface=eth1
lnode=pod-c1
rnode=pod-c2
lvolume=/dev/sdb
rvolume=/dev/sdc
skip_mkfs=false
net_type=tcp

while true; do
    case "$1" in
        -h|--help)           usage; exit ;;
        --ip1)               ip1=$2; shift 2 ;;
        --ip2)               ip2=$2; shift 2 ;;
        -i|--interface)      iface=$2; shift 2 ;;
        --left-node)         lnode=$2; shift 2 ;;
        --right-node)        rnode=$2; shift 2 ;;
        --left-volume)       lvolume=$2; shift 2 ;;
        --right-volume)      rvolume=$2; shift 2 ;;
        --skip-mkfs)         skip_mkfs=true; shift 1 ;;
        --net-type)          net_type=$2; shift 2 ;;
        --)                  shift; break ;;
        *)                   break ;;
    esac
done

cdf=${1:-}
argsfile=${2:-}

hare_dir=/var/lib/hare

die() {
    echo "$PROG: ERROR: $*" >&2
    exit 1
}

if [[ -f $argsfile ]]; then
    while IFS=': ' read name value; do
       case $name in
           ip1)          ip1=$value     ;;
           ip2)          ip2=$value     ;;
           interface)    iface=$value   ;;
           left-node)    lnode=$value   ;;
           right-node)   rnode=$value   ;;
           left-volume)  lvolume=$value ;;
           right-volume) rvolume=$value ;;
           skip-mkfs)
               [[ $value == true || $value == false ]] ||
                   die 'Invalid value of `skip-mkfs` parameter.
Supported values: true, false'
               skip_mkfs=$value ;;
           net-type)     net_type=$value ;;
           '')           ;;
           *) echo "Invalid parameter '$name' in $argsfile" >&2
              usage >&2; exit 1 ;;
       esac
    done < $argsfile
fi

[[ $ip1 ]] && [[ $ip2 ]] && [[ $cdf ]]  || {
    usage >&2
    exit 1
}

[[ -b $lvolume ]] || die "meta-data volume $lvolume is not available"
[[ -b $rvolume ]] || die "meta-data volume $rvolume is not available"

netmask=$(ip -oneline -4 address show dev $iface |
              awk '{print $4}' | cut -d/ -f2)

echo 'Adding the roaming IP addresses into Pacemaker...'
sudo pcs cluster cib icfg
sudo pcs -f icfg resource create ip-c1 ocf:heartbeat:IPaddr2 \
         ip=$ip1 cidr_netmask=$netmask iflabel=c1 op monitor interval=0
sudo pcs -f icfg resource create ip-c2 ocf:heartbeat:IPaddr2 \
         ip=$ip2 cidr_netmask=$netmask iflabel=c2 op monitor interval=0
sudo pcs -f icfg constraint location ip-c1 prefers $lnode
sudo pcs -f icfg constraint location ip-c2 prefers $rnode
sudo pcs cluster cib-push icfg --config

echo 'Adding LNet...'
sudo pcs cluster cib lcfg
sudo pcs -f lcfg resource create lnet systemd:lnet
sudo pcs -f lcfg resource clone lnet
sudo pcs cluster cib-push lcfg --config

run_on_both() {
    local cmd=$*
    eval $cmd
    ssh $rnode $cmd
}

sudo pcs cluster cib lcfg
sudo pcs -f lcfg resource create lnet-c1 ocf:eos:lnet \
         iface=$iface:c1 nettype=$net_type op monitor interval=30s
sudo pcs -f lcfg resource create lnet-c2 ocf:eos:lnet \
         iface=$iface:c2 nettype=$net_type op monitor interval=30s
sudo pcs -f lcfg resource group add c1 ip-c1 lnet-c1
sudo pcs -f lcfg resource group add c2 ip-c2 lnet-c2
sudo pcs -f lcfg constraint order lnet-clone then lnet-c1
sudo pcs -f lcfg constraint order lnet-clone then lnet-c2
sudo pcs cluster cib-push lcfg --config

# Give Pacemaker time to configure the IPs
for i in {1..10}; do
    sleep 5
    sudo lctl list_nids | grep -qF $ip1 || continue
    ssh $rnode "sudo lctl list_nids | grep -qF $ip2" || continue
    break
done

# Check the IPs
check_msg="
Check the following:
 1) Make sure the netmask of the main IP on $iface interface is <= 24 bits.
 2) Run 'pcs status' and make sure LNet is configured.
 3) STONITH is configured or disabled in Pacemaker."

ip a | grep -qF $ip1 ||
  die "IP address $ip1 doesn't appear to be configured at $lnode. $check_msg"

ssh $rnode "ip a | grep -qF $ip2" ||
  die "IP address $ip2 doesn't appear to be configured at $rnode. $check_msg"

sudo lctl list_nids | grep -qF $ip1 ||
  die "LNet endpoint $ip1 doesn't appear to be configured at $lnode. $check_msg"

ssh $rnode "sudo lctl list_nids | grep -qF $ip2" ||
  die "LNet endpoint $ip2 doesn't appear to be configured at $rnode. $check_msg"

if ! $skip_mkfs; then
    sudo mkfs.ext4 -q $lvolume >/dev/null <<< y
    sudo mkfs.ext4 -q $rvolume >/dev/null <<< y
fi

# Mount /var/mero (if not mounted).
mkdir -p /var/mero &&
  ! mountpoint -q /var/mero && sudo mount $lvolume /var/mero || true
ssh $rnode "mkdir -p /var/mero &&
  ! mountpoint -q /var/mero && sudo mount $rvolume /var/mero || true"

echo 'Preparing Hare configuration files...'

# Update data_iface values in CDF: 1st data_iface will be `${iface}_c1`,
#                                  2nd data_iface will be `${iface}_c2`.
sudo sed -r -e "/[#].*data_iface/b ; # skip commented out data_iface lines
                /data_iface: *[a-z0-9]+[_:]c[12]/b ; # skip already updated
                0,/(data_iface: *)[a-z0-9]+\b/s//\1${iface}_c1/ ;
                0,/(data_iface: *)[a-z0-9]+\b/s//\1${iface}_c2/" \
            -i $cdf

if facter --version | grep -q ^3; then
    # New facter-3 requires colons (:) for interface aliases.
    sudo sed -r -e "/[#].*data_iface/b ; # skip commented out data_iface lines
                    s/(data_iface: *${iface})_(c[12])/\1:\2/" \
            -i $cdf
fi

# Make sure mero-kernel is not loaded.
run_on_both 'sudo systemctl stop mero-kernel'

hctl bootstrap --mkfs $cdf
hctl shutdown

# LNet endpoints suffixes should be unique so that in case
# of a failover all the Mero services (which would work on
# the same node) could talk to each other.
# (It is a workaround for Mero EOS-2799 issue.)
for f in $hare_dir/{confd.xc,consul-kv.json}; do
    sed -r -e "s/($ip2.*:12345:1):1/\1:2/" \
           -e "s/($ip2.*:12345:2):1/\1:3/" \
           -e "s/($ip2.*:12345:2):2/\1:4/" \
        -i $f
done

run_on_both 'sudo rm -f /etc/sysconfig/m0d-*'

hctl bootstrap --conf-dir $hare_dir
hctl shutdown

# Create /var/mero dirs for the foreign data-stacks:
sudo mkdir -p /var/mero2
ssh $rnode 'sudo mkdir -p /var/mero1'

# Prepare Mero conf files on each node:
lm0confs=$(echo /etc/sysconfig/m0d-*)
rm0confs=$(sudo ssh $rnode 'echo /etc/sysconfig/m0d-*')
sudo scp -q $lm0confs $rnode:/etc/sysconfig/
sudo scp -q $rnode:\{${rm0confs/ /,}\} /etc/sysconfig/
for f in $rm0confs; do sudo sed '1iMERO_M0D_DATA_DIR=/var/mero2' -i $f; done
ssh $rnode "for f in $lm0confs; do \
                       sudo sed '1iMERO_M0D_DATA_DIR=/var/mero1' -i \$f; done"


echo 'Preparing Consul agents config files...'
cmd='
sudo cp /usr/lib/systemd/system/hare-consul-agent.service
        /usr/lib/systemd/system/hare-consul-agent-c1.service &&
sudo cp /usr/lib/systemd/system/hare-consul-agent.service
        /usr/lib/systemd/system/hare-consul-agent-c2.service &&
for i in c{1,2}; do
    sudo sed "s/consul-env/&-$i/"
             -i /usr/lib/systemd/system/hare-consul-agent-$i.service &&
    sudo sed "/ExecStart=/aExecStartPost=/bin/sleep 5"
             -i /usr/lib/systemd/system/hare-consul-agent-$i.service;
done'
run_on_both $cmd

cmd_deregister_node() {
    local node=$1
    cat <<EOF
/usr/bin/curl -i -X PUT -d '{\\"Node\\":\\"$node\\"}' \
    http://localhost:8500/v1/catalog/deregister
EOF
}

# Cleanup Consul data directory before starting the agent to
# avoid possible inconsistencies related to consensus when Consul
# agent joins the Consul cluster. Mainly problems related to leader
# election.

consul_c2_cfg_dir=$hare_dir/consul-$ip2
consul_c1_cfg_dir=$hare_dir/consul-$ip1

sudo sed \
 -e "/ExecStart=/iExecStartPre=/bin/rm -rf $consul_c2_cfg_dir" \
 -i /usr/lib/systemd/system/hare-consul-agent-c2.service
sudo systemctl daemon-reload

cmd="
sudo sed
 -e '/ExecStart=/iExecStartPre=/bin/rm -rf $consul_c1_cfg_dir'
 -i /usr/lib/systemd/system/hare-consul-agent-c1.service &&
sudo systemctl daemon-reload"
ssh $rnode $cmd

unset consul_c1_cfg_dir consul_c2_cfg_dir

sudo cp $hare_dir/consul-env $hare_dir/consul-env-c1
scp $rnode:$hare_dir/consul-env $hare_dir/consul-env-c2
sudo sed -r \
  -e 's/server$/server-c1/' \
  -e "s/JOIN=/&-retry-join $ip2 /" \
  -i $hare_dir/consul-env-c1
sudo sed -r \
  -e 's/server$/server-c2/' \
  -e 's/127.0.0.1 //' \
  -i $hare_dir/consul-env-c2

scp $hare_dir/consul-env $rnode:$hare_dir/consul-env-c1
cmd="
sudo cp $hare_dir/consul-env $hare_dir/consul-env-c2 &&
sudo sed -r \
  -e 's/server$/server-c1/' \
  -e 's/127.0.0.1 //' \
  -e 's/JOIN=/&-retry-join $ip2 /' \
  -e 's/ -bootstrap-expect 1//' \
  -i $hare_dir/consul-env-c1 &&
sudo sed -r \
  -e 's/server$/server-c2/' \
  -i $hare_dir/consul-env-c2"
ssh $rnode $cmd

run_on_both "mkdir -p $hare_dir/consul-server-c{1,2}-conf"

conf_dir=consul-server-c1-conf

sudo cp $hare_dir/consul-server-conf/consul-server-conf.json \
        $hare_dir/$conf_dir/$conf_dir.json
sudo sed -e 's/"--hax"/"--svc", "hare-hax-c1"/' \
         -i $hare_dir/$conf_dir/$conf_dir.json

cp $hare_dir/$conf_dir/$conf_dir.json /tmp/$conf_dir.json
sudo sed -e '/\"server\"/a\ \ "leave_on_terminate": true,' \
         -i /tmp/$conf_dir.json
scp /tmp/$conf_dir.json $rnode:$hare_dir/$conf_dir/
rm /tmp/$conf_dir.json

conf_dir=consul-server-c2-conf

cmd="
sudo cp $hare_dir/consul-server-conf/consul-server-conf.json \
        $hare_dir/$conf_dir/$conf_dir.json &&
sudo sed -e 's/\"--hax\"/\"--svc\", \"hare-hax-c2\"/' \
         -i $hare_dir/$conf_dir/$conf_dir.json &&

cp $hare_dir/$conf_dir/$conf_dir.json /tmp/$conf_dir.json &&
sudo sed -e '/\"server\"/a\ \ \"leave_on_terminate\": true,' \
         -i /tmp/$conf_dir.json"

ssh $rnode $cmd
scp $rnode:/tmp/$conf_dir.json $hare_dir/$conf_dir/
ssh $rnode "rm /tmp/$conf_dir.json"

unset conf_dir

echo 'Adding Consul to Pacemaker...'
sudo pcs resource create consul-c1 systemd:hare-consul-agent-c1
sudo pcs resource create consul-c2 systemd:hare-consul-agent-c2
sudo pcs resource group add c1 consul-c1 --after ip-c1
sudo pcs resource group add c2 consul-c2 --after ip-c2

echo 'Adding Mero kernel module to Pacemaker...'
sudo pcs resource create mero-kernel systemd:mero-kernel clone
sudo pcs constraint order lnet-c1 then mero-kernel-clone
sudo pcs constraint order lnet-c2 then mero-kernel-clone

echo 'Adding Hax to Pacemaker...'

sudo cp /usr/lib/systemd/system/hare-hax.service \
        /usr/lib/systemd/system/hare-hax-c1.service
sudo cp /usr/lib/systemd/system/hare-hax.service \
        /usr/lib/systemd/system/hare-hax-c2.service
sudo sed -e 's/hare-consul-agent.service/hare-consul-agent-c1.service/' \
         -e "/ExecStart=/iExecStartPre=/bin/sh -c 'mountpoint /var/mero || /bin/mount $lvolume /var/mero'" \
         -e "/ExecStart=/aExecStopPost=/bin/sh -c '! mountpoint /var/mero || while ! /bin/umount /var/mero; do lsof +D /var/mero; sleep 1; done'" \
         -i /usr/lib/systemd/system/hare-hax-c1.service
sudo sed -e 's/hare-consul-agent.service/hare-consul-agent-c2.service/' \
         -e 's;ExecStart.*cd /var/mero;&2;' \
         -e "/ExecStart/iEnvironmentFile=$hare_dir/hax-env-c2" \
         -e "/ExecStart=/iExecStartPre=/bin/sh -c 'mountpoint /var/mero2 || /bin/mount $rvolume /var/mero2'" \
         -e "/ExecStart=/aExecStopPost=/bin/sh -c '! mountpoint /var/mero2 || while ! /bin/umount /var/mero2; do lsof +D /var/mero2; sleep 1; done'" \
         -i /usr/lib/systemd/system/hare-hax-c2.service
echo "HARE_HAX_NODE_NAME=$rnode" | sudo tee $hare_dir/hax-env-c2 > /dev/null

cmd="
sudo cp /usr/lib/systemd/system/hare-hax.service
        /usr/lib/systemd/system/hare-hax-c1.service &&
sudo sed -e 's/hare-consul-agent.service/hare-consul-agent-c1.service/'
         -e 's;ExecStart.*cd /var/mero;&1;' \
         -e '/ExecStart/iEnvironmentFile=$hare_dir/hax-env-c1'
         -e \"/ExecStart=/iExecStartPre=/bin/sh -c 'mountpoint /var/mero1 || /bin/mount $lvolume /var/mero1'\"
         -e \"/ExecStart=/aExecStopPost=/bin/sh -c '! mountpoint /var/mero1 || while ! /bin/umount /var/mero1; do lsof +D /var/mero1; sleep 1; done'\"
         -i /usr/lib/systemd/system/hare-hax-c1.service &&
sudo cp /usr/lib/systemd/system/hare-hax.service
        /usr/lib/systemd/system/hare-hax-c2.service &&
sudo sed -e 's/hare-consul-agent.service/hare-consul-agent-c2.service/'
         -e \"/ExecStart=/iExecStartPre=/bin/sh -c 'mountpoint /var/mero || /bin/mount $rvolume /var/mero'\"
         -e \"/ExecStart=/aExecStopPost=/bin/sh -c '! mountpoint /var/mero || while ! /bin/umount /var/mero; do lsof +D /var/mero; sleep 1; done'\"
         -i /usr/lib/systemd/system/hare-hax-c2.service &&
echo 'HARE_HAX_NODE_NAME=$lnode' | sudo tee $hare_dir/hax-env-c1 > /dev/null"
ssh $rnode $cmd

sudo pcs cluster cib mcfg
sudo pcs -f mcfg resource create hax-c1 systemd:hare-hax-c1 op stop timeout=30
sudo pcs -f mcfg resource create hax-c2 systemd:hare-hax-c2 op stop timeout=30
sudo pcs -f mcfg resource group add c1 hax-c1
sudo pcs -f mcfg resource group add c2 hax-c2
sudo pcs -f mcfg constraint order mero-kernel-clone then hax-c1
sudo pcs -f mcfg constraint order mero-kernel-clone then hax-c2
sudo pcs -f mcfg constraint order consul-c2 then hax-c1
sudo pcs -f mcfg constraint order consul-c1 then hax-c2
sudo pcs cluster cib-push mcfg --config

echo 'Adding Mero to Pacemaker...'

cmd='
sudo sed "s/TimeoutStopSec=.*/TimeoutStopSec=5sec/"
    -i /usr/lib/systemd/system/m0d@.service &&
sudo systemctl daemon-reload'
run_on_both $cmd

get_fid() {
    local node=$1
    local svc=$2

    local conf_dir=consul-server-$node-conf

    jq -r '.services[] | "\(.name) \(.checks[].args[])"' \
       $hare_dir/$conf_dir/$conf_dir.json | awk "/$svc.0x/ {print \$2}"
}

c1_confd=$(get_fid c1 confd)
c2_confd=$(get_fid c2 confd)
c1_ios=$(get_fid c1 ios)
c2_ios=$(get_fid c2 ios)

sudo pcs cluster cib mcfg
sudo pcs -f mcfg resource create mero-confd-c1 systemd:m0d@$c1_confd op stop \
     timeout=600
sudo pcs -f mcfg resource create mero-ios-c1 systemd:m0d@$c1_ios op stop \
     timeout=600
sudo pcs -f mcfg resource group add c1 mero-confd-c1
sudo pcs -f mcfg resource group add c1 mero-ios-c1
sudo pcs -f mcfg resource create mero-confd-c2 systemd:m0d@$c2_confd op stop \
     timeout=600
sudo pcs -f mcfg resource create mero-ios-c2 systemd:m0d@$c2_ios op stop \
     timeout=600
sudo pcs -f mcfg resource group add c2 mero-confd-c2
sudo pcs -f mcfg resource group add c2 mero-ios-c2
sudo pcs -f mcfg constraint order mero-confd-c1 then mero-ios-c2
sudo pcs -f mcfg constraint order mero-confd-c2 then mero-ios-c1
sudo pcs cluster cib-push mcfg --config

is_virtual() {
    which facter &>/dev/null || return 1  # assume physical if facter is missing
    [[ $(facter is_virtual) == true ]] && return 0 || return 1
}

if ! is_virtual; then
    echo 'Configuring ClusterIP constrains in Pacemaker...'
    sudo pcs cluster cib clustercfg
    sudo pcs -f clustercfg constraint order c1 then ClusterIP-clone
    sudo pcs -f clustercfg constraint order c2 then ClusterIP-clone
    sudo pcs cluster cib-push clustercfg --config
fi

echo 'Disabling some systemd units...'
units_to_disable=(
    elasticsearch
    haproxy
    rabbitmq-server
    statsd
    slapd
    s3authserver
)

for u in ${units_to_disable[@]}; do
    run_on_both "sudo systemctl stop $u && sudo systemctl disable $u" || true
done

echo 'Adding ldap to Pacemaker...'
sudo pcs resource create ldap systemd:slapd clone op monitor interval=30s

echo 'Adding s3authserver to Pacemaker...'
sudo pcs cluster cib s3authcfg
sudo pcs -f s3authcfg resource create s3auth systemd:s3authserver clone op \
    monitor interval=30
sudo pcs -f s3authcfg constraint order ldap-clone then s3auth-clone
sudo pcs -f s3authcfg constraint order mero-ios-c1 then s3auth-clone
sudo pcs -f s3authcfg constraint order mero-ios-c2 then s3auth-clone
sudo pcs cluster cib-push s3authcfg --config

echo 'Adding elastic search to Pacemaker..'
sudo pcs resource create els-search systemd:elasticsearch clone op \
    monitor interval=30s

echo 'Adding statsd to Pacemaker...'
sudo pcs resource create statsd systemd:statsd clone op monitor interval=30s
sudo pcs constraint order els-search-clone then statsd-clone

echo 'Adding haproxy to pacemaker...'
sudo pcs resource create haproxy-c1 systemd:haproxy op monitor interval=30s
sudo pcs resource create haproxy-c2 systemd:haproxy op monitor interval=30s
sudo pcs constraint location haproxy-c1 prefers $lnode=INFINITY
sudo pcs constraint location haproxy-c1 avoids $rnode=INFINITY
sudo pcs constraint location haproxy-c2 prefers $rnode=INFINITY
sudo pcs constraint location haproxy-c2 avoids $lnode=INFINITY

echo 'Adding S3server to Pacemaker...'
get_s3_svc() {
    local cfg=$1
    local svc=$2
    jq -r '.services[] | "\(.name) \(.checks[].args[])"' $cfg |
        awk "/$svc.s3server@/ {print \$2}"
}

echo 'Adding rabbit-mq resources and constraints...'
sudo pcs resource create rabbitmq systemd:rabbitmq-server clone op \
    monitor interval=30s meta failure-timeout=20s

echo 'Adding s3background services...'
sudo pcs cluster cib s3bcfg
sudo pcs -f s3bcfg resource create s3backcons-c1 systemd:s3backgroundconsumer
sudo pcs -f s3bcfg resource create s3backcons-c2 systemd:s3backgroundconsumer
sudo pcs -f s3bcfg constraint location s3backcons-c1 prefers $lnode=INFINITY
sudo pcs -f s3bcfg constraint location s3backcons-c1 avoids $rnode=INFINITY
sudo pcs -f s3bcfg constraint location s3backcons-c2 prefers $rnode=INFINITY
sudo pcs -f s3bcfg constraint location s3backcons-c2 avoids $lnode=INFINITY
sudo pcs -f s3bcfg constraint order rabbitmq-clone then s3backcons-c1
sudo pcs -f s3bcfg constraint order rabbitmq-clone then s3backcons-c2
sudo pcs -f s3bcfg resource create s3backprod systemd:s3backgroundproducer op \
    monitor interval=30s
sudo pcs -f s3bcfg constraint order rabbitmq-clone then s3backprod
sudo pcs -f s3bcfg constraint colocation add s3backprod with s3backcons-c1 \
    score=50000
sudo pcs -f s3bcfg constraint colocation add s3backprod with s3backcons-c2 \
    score=50000
sudo pcs cluster cib-push s3bcfg --config

s3servers_all=

add_s3server_resources() {
   local suffix=$1
   local node_local=$2
   local node_remote=$3
   local s3servers=

   local conf_dir=consul-server-$suffix-conf
   local s3server_fids=(
       $(get_s3_svc $hare_dir/$conf_dir/$conf_dir.json s3service)
   )
   ((${#s3server_fids[@]} == 0)) && return

   sudo pcs cluster cib s3cfg
   local count=1
   for i in ${s3server_fids[@]}; do
      sudo pcs -f s3cfg resource create s3server-$suffix-$count systemd:$i op \
          stop timeout=600
      sudo pcs -f s3cfg constraint location s3server-$suffix-$count \
          prefers $node_local=INFINITY
      sudo pcs -f s3cfg constraint location s3server-$suffix-$count \
          avoids $node_remote=INFINITY
      # Order constraint adds the startup dependency of s3server on s3authserver
      sudo pcs -f s3cfg constraint \
           order s3auth-clone then s3server-$suffix-$count
      # Colocation constraint will add a s3server's liveness dependency
      # on the local s3authserver
      sudo pcs -f s3cfg constraint colocation add s3server-$suffix-$count \
          with s3auth-clone score=INFINITY
      s3servers+=" s3server-$suffix-$count"
      s3servers_all+=" s3server-$suffix-$count"
      (( count++ ))
   done
   sudo pcs cluster cib-push s3cfg --config

   sudo pcs cluster cib s3cfg
   sudo pcs -f s3cfg constraint order set $s3servers require-all=false \
       sequential=false set s3backcons-$suffix
   sudo pcs -f s3cfg constraint order set $s3servers require-all=false \
       sequential=false set haproxy-$suffix
   sudo pcs cluster cib-push s3cfg --config
}

cmd='
sudo sed "s/TimeoutStopSec=.*/TimeoutStopSec=7sec/"
         -i /usr/lib/systemd/system/s3server@.service &&
sudo systemctl daemon-reload'
run_on_both $cmd

add_s3server_resources c1 $lnode $rnode
add_s3server_resources c2 $rnode $lnode

sudo pcs cluster cib s3cfg
sudo pcs -f s3cfg constraint order set $s3servers_all require-all=false \
    sequential=false set s3backprod
sudo pcs cluster cib-push s3cfg --config

echo 'Adding mero-free-space-monitor to pacemaker...'
sudo pcs resource create mero-free-space-mon systemd:mero-free-space-monitor \
     op monitor interval=30s
sudo pcs constraint order mero-ios-c1 then mero-free-space-mon
sudo pcs constraint order mero-ios-c2 then mero-free-space-mon
