#!/usr/bin/env bash
set -eu -o pipefail
# set -x
export PS4='+ [${BASH_SOURCE[0]##*/}:${LINENO}${FUNCNAME[0]:+:${FUNCNAME[0]}}] '

# :help: bootstrap the cluster

PROG=${0##*/}

usage() {
    cat <<EOF
Usage: $PROG [OPTIONS...] --conf-dir DIR|cluster-description-file.yaml

Bootstrap cluster described by the description file or
using the ready configuration at the specified directory.

Options:
  --mkfs               Do m0mkfs (WARNING: wipes all Mero data).
  -c, --conf-dir DIR   Don't generate configuration files, use
                       existing ones at the specified DIR(ectory).
  -h, --help           Show this help and exit.
EOF
}

die() {
    echo "$PROG: $*" >&2
    exit 1
}

say() {
    echo -n "$(date '+%F %T'): $*"
}

get_server_nodes() {
    jq -r '.servers[] | "\(.hostname) \(.ipaddr)"' $cfgen_out/consul-agents.json
}

get_client_nodes() {
    jq -r '.clients[] | "\(.hostname) \(.ipaddr)"' $cfgen_out/consul-agents.json
}

get_all_nodes() {
    jq -r '(.servers + .clients)[] | "\(.hostname) \(.ipaddr)"' \
        $cfgen_out/consul-agents.json
}

get_session() {
    consul kv get -detailed leader | awk '/Session/ {print $2}'
}

get_session_checks_nr() {
    local sid=$1
    curl -sX GET http://localhost:8500/v1/session/info/$sid |
        jq -r '.[].Checks|length'
}

wait_rc_leader() {
    local count=1
    while [[ $(get_session) == '-' ]]; do
        if (( $count > 5 )); then
            consul kv put leader elect$RANDOM > /dev/null
            count=1
        fi
        sleep 1
        echo -n '.'
        (( count++ ))
    done
}

wait4() {
    for pid in $*; do
        wait $pid
    done
}

get_ready_agents() {
    consul members | sed 1d | awk '{print $1}'
}

get_ready_agents_nr() {
    consul members | sed 1d | wc -l
}

abort_if_RC_leader_election_is_impossible() {
    local ssh node confd_id svc cmd
    local status_commands=()

    # This code assumes that there is at most one confd server per node.
    # This assumption is enforced by `cfgen`.
    while IFS=/ read node confd_id; do
        local ok=true
        confd_id="m0d@$(printf 0x7200000000000001:0x%x $confd_id)"
        if [[ $node == $this_node_name ]]; then
            ssh=
        else
            ssh="ssh $node"
        fi

        for svc in hare-hax $confd_id; do
            if $ssh sudo systemctl --quiet --state=failed is-failed $svc; then
                ok=false
                status_commands+=(
                    "${ssh:+$ssh }sudo systemctl status $svc"
                )
            fi
        done
        if $ok; then
            # RC Leader session can be created on at least one Consul server
            # node.
            return 0
        fi
    done < <(jq -r '.[] | .key' $cfgen_out/consul-kv.json |
                grep '/services/confd$' |
                # m0conf/nodes/<hostname>/processes/<process_id>/services/confd
                cut -d/ -f3,5
            )
    cat >&2 <<EOF
**ERROR** RC leader election is guaranteed to fail.

The RC leader can only be elected if there is a Consul server node
on which both hare-hax and confd m0d@ service are in "non-failed" state.

To see details, type
$(for cmd in "${status_commands[@]}"; do
    echo "    $cmd"
done)

To reset the "failed" state, type
$(for cmd in "${status_commands[@]}"; do
    echo "    ${cmd/ status/ reset-failed}"
done)
EOF
    exit 1
}

TEMP=$(getopt --options hc: \
              --longoptions help,mkfs,conf-dir: \
              --name "$PROG" -- "$@" || true)

(($? == 0)) || { usage >&2; exit 1; }

eval set -- "$TEMP"

conf_dir=
opt_mkfs=

while true; do
    case "$1" in
        -h|--help)           usage; exit ;;
        --mkfs)              opt_mkfs=--mkfs; shift ;;
        -c|--conf-dir)       conf_dir=$2; shift 2 ;;
        --)                  shift; break ;;
        *)                   break ;;
    esac
done

[[ $conf_dir ]] || (($# == 1)) || {
    usage >&2
    exit 1
}

cluster_descr=${1:-}
cfgen_out=${conf_dir:-'/var/lib/hare'}
this_node_name=$(cat /etc/salt/minion_id)

if sudo systemctl --quiet is-active hare-consul-agent; then
    die 'hare-consul-agent is active ==> cluster is already running'
fi

if ! [[ $conf_dir ]]; then
    if ! [[ -d $cfgen_out ]]; then
        cat <<EOF >&2
$cfgen_out directory does not exist.
Try reinstalling hare.
EOF
        exit 1
    fi
    if ! [[ -w $cfgen_out ]]; then
        cat <<EOF >&2
Cannot write to $cfgen_out directory.

Did you forget to add current user ($USER) to 'hare' group?
If so, run
    sudo usermod --append --groups hare $USER
then re-login and try to bootstrap again.
EOF
        exit 1
    fi
    say 'Generating cluster configuration...'
    cfgen -o $cfgen_out $cluster_descr
    dhall text < $cfgen_out/confd.dhall | m0confgen > $cfgen_out/confd.xc
    echo ' OK'
fi

abort_if_RC_leader_election_is_impossible

# Get my IP address (the one that the other agents will join to).
read _ join_ip <<< $(get_server_nodes | grep -w $this_node_name)

if [[ -z $join_ip ]]; then
    cat <<'EOF' >&2
Bootstrap should be executed on a Consul server node.

"Consul server nodes" are the nodes with 'runs_confd: true' in the CDF
(cluster description file).
EOF
    exit 1
fi

say 'Starting Consul server agent on this node...'
# $join_ip is our bind_ip address
mk-consul-env --mode server --bind $join_ip \
              --extra-options '-ui -bootstrap-expect 1'

sudo systemctl start hare-consul-agent

# Wait for Consul's internal leader to be ready.
# (Until then the KV store won't be accessible.)
while ! consul info 2>/dev/null | grep -q 'leader.*true'; do
    sleep 1
    echo -n '.'
done
echo ' OK'

say 'Importing configuration into the KV store...'
jq '[.[] | {key, value: (.value | @base64)}]' < $cfgen_out/consul-kv.json |
    consul kv import - > /dev/null
echo ' OK'

say 'Starting Consul agents on other cluster nodes...'
pids=()
while read node bind_ip; do
    ssh $node "$(which mk-consul-env) --mode server \
                                      --bind $bind_ip --join $join_ip &&
               sudo systemctl start hare-consul-agent" &
    pids+=($!)
done < <(get_server_nodes | grep -vw $this_node_name || true)

while read node bind_ip; do
    ssh $node "$(which mk-consul-env) --mode client \
                                      --bind $bind_ip --join $join_ip &&
               sudo systemctl start hare-consul-agent" &
    pids+=($!)
done < <(get_client_nodes)
wait4 ${pids[@]-}
agents_nr=$(( ${#pids[@]} + 1 ))

# Waiting for the agents to get ready...
count=1
while (( $(get_ready_agents_nr) != $agents_nr )); do
    if (( $count > 5 )); then
        echo 'Some agent(s) failed to start in due time:' >&2
        diff <(get_ready_agents | sort) \
             <(get_all_nodes | awk '{print $1}' | sort) | sed 1d >&2
        echo 'Check connectivity and firewall (Consul ports must be opened)' >&2
        exit 1
    fi
    echo -n '.'
    sleep 1
    (( count++ ))
done
echo ' OK'

say 'Updating Consul agents configs from the KV store...'
update-consul-conf &
pids=($!)
while read node _; do
    ssh $node "PATH=$PATH $(which update-consul-conf)" &
    pids+=($!)
done < <(get_all_nodes | grep -vw $this_node_name || true)
wait4 ${pids[@]}
echo ' OK'

say 'Installing Mero configuration files...'
while read node _; do
    scp -q $cfgen_out/confd.xc $node:$cfgen_out
done < <(get_server_nodes | grep -vw $this_node_name || true)
echo ' OK'

say 'Waiting for the RC Leader to get elected...'
wait_rc_leader
sid=$(get_session)
# There is always the serfHealth check in the session. But
# if it is the only one - we should destroy the current session
# (and wait for re-election to happen) to make sure that the new
# session will be bound to the Mero services checks also.
while (( $(get_session_checks_nr $sid) == 1 )); do
    curl -sX PUT http://localhost:8500/v1/session/destroy/$sid &>/dev/null
    wait_rc_leader
    sid=$(get_session)
done
echo ' OK'

get_nodes() {
    local phase=$1

    if [[ $phase == phase1 ]]; then
        # Note: confd-s are running on server nodes only.
        get_server_nodes
    else
        get_all_nodes
    fi
}

start_mero() {
    local op=$1
    local phase=$2

    say "Starting Mero ($phase, $op)..."
    [[ $op == 'mkfs' ]] && op='--mkfs-only' || op=
    bootstrap-node $op --phase $phase &
    pids=($!)

    while read node _; do
        ssh $node "PATH=$PATH $(which bootstrap-node) $op --phase $phase" &
        pids+=($!)
    done < <(get_nodes $phase | grep -vw $this_node_name || true)
    wait4 ${pids[@]}
    echo ' OK'
}

# Start Mero in two phases: 1st confd-s, then ios-es.
bootstrap_nodes() {
    local phase=$1

    if [[ $opt_mkfs ]]; then
        start_mero 'mkfs' $phase
    fi
    start_mero 'm0d' $phase
}

# Start confds first
bootstrap_nodes phase1

# Start ioservices
bootstrap_nodes phase2

. update-consul-conf --dry-run  # import S3_IDs
if [[ -n $S3_IDs ]]; then
    # Now the 3rd phase (s3servers).
    say 'Starting S3 servers (phase3)...'
    bootstrap-node --phase phase3 &
    pids=($!)

    while read node _; do
        ssh $node "PATH=$PATH $(which bootstrap-node) --phase phase3" &
        pids+=($!)
    done < <(get_all_nodes | grep -vw $this_node_name || true)
    wait4 ${pids[@]}
    echo ' OK'
fi

say 'Checking health of the services...'
check_service() {
    local svc=$1
    curl -s http://127.0.0.1:8500/v1/health/service/$svc |
        jq -r '.[] | "\(.Node.Node) \([.Checks[].Status]|unique)"' |
        fgrep -v '["passing"]' || true
}
count=1
for svc in confd ios s3service; do
    svc_not_ready=$(check_service $svc)
    while [[ $svc_not_ready ]]; do
        if (( $count > 30 )); then
            echo $svc_not_ready >&2
            echo "Check '$svc' service on the node(s) listed above." >&2
            exit 1
        fi
        (( count++ ))
        sleep 1
        svc_not_ready=$(check_service $svc)
    done
done
echo ' OK'
